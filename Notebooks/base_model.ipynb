{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, AutoConfig\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "LlamaForCausalLM(\n",
       "  (model): LlamaModel(\n",
       "    (embed_tokens): Embedding(32256, 2048)\n",
       "    (layers): ModuleList(\n",
       "      (0-23): 24 x LlamaDecoderLayer(\n",
       "        (self_attn): LlamaSdpaAttention(\n",
       "          (q_proj): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "          (k_proj): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "          (v_proj): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "          (o_proj): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "          (rotary_emb): LlamaLinearScalingRotaryEmbedding()\n",
       "        )\n",
       "        (mlp): LlamaMLP(\n",
       "          (gate_proj): Linear(in_features=2048, out_features=5504, bias=False)\n",
       "          (up_proj): Linear(in_features=2048, out_features=5504, bias=False)\n",
       "          (down_proj): Linear(in_features=5504, out_features=2048, bias=False)\n",
       "          (act_fn): SiLU()\n",
       "        )\n",
       "        (input_layernorm): LlamaRMSNorm()\n",
       "        (post_attention_layernorm): LlamaRMSNorm()\n",
       "      )\n",
       "    )\n",
       "    (norm): LlamaRMSNorm()\n",
       "  )\n",
       "  (lm_head): Linear(in_features=2048, out_features=32256, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"deepseek-ai/deepseek-coder-1.3b-instruct\", trust_remote_code=True)\n",
    "config = AutoConfig.from_pretrained(\"./\")\n",
    "model = AutoModelForCausalLM.from_pretrained(\"./pytorch_model.bin\", config=config)\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:32021 for open-end generation.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"Generate sql query to find the number of students whose name is john?\\n\\n\\nSELECT COUNT(*) \\nFROM students \\nWHERE name = 'john';\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\""
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt = \"Generate sql query to find the number of students whose name is john?\"\n",
    "inputs = tokenizer(prompt, return_tensors=\"pt\")\n",
    "generate_ids = model.generate(inputs.input_ids, max_length=50)\n",
    "tokenizer.batch_decode(generate_ids, skip_special_tokens=True, clean_up_tokenization_spaces=False)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_model_details_to_file(model, filename=\"architecture.txt\"):\n",
    "    total_params = 0\n",
    "    total_layers = 0\n",
    "    lines = []\n",
    "    lines.append(\"Model Layers and Parameters:\\n\\n\")\n",
    "    for name, param in model.named_parameters():\n",
    "        total_layers += 1\n",
    "        num_params = param.numel()\n",
    "        total_params += num_params\n",
    "        lines.append(f\"Layer {total_layers}: {name}\\n\")\n",
    "        lines.append(f\" - Shape: {tuple(param.shape)}\\n\")\n",
    "        lines.append(f\" - Data type: {param.dtype}\\n\")\n",
    "        lines.append(f\" - Number of parameters: {num_params}\\n\\n\")\n",
    "    lines.append(f\"Total number of layers: {total_layers}\\n\")\n",
    "    lines.append(f\"Total number of parameters: {total_params}\\n\")\n",
    "    lines.append(f\"Model data-type precision: {next(model.parameters()).dtype}\\n\")\n",
    "    \n",
    "    with open(filename, 'w') as f:\n",
    "        f.writelines(lines)\n",
    "    print(f\"Model details have been written to {filename}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_model_details(model):\n",
    "    total_params = 0\n",
    "    total_layers = 0\n",
    "    print(\"Model Layers and Parameters:\\n\")\n",
    "    for name, param in model.named_parameters():\n",
    "        total_layers += 1\n",
    "        num_params = param.numel()\n",
    "        total_params += num_params\n",
    "        print(f\"Layer {total_layers}: {name}\")\n",
    "        print(f\" - Shape: {tuple(param.shape)}\")\n",
    "        print(f\" - Data type: {param.dtype}\")\n",
    "        print(f\" - Number of parameters: {num_params}\\n\")\n",
    "    print(f\"Total number of layers: {total_layers}\")\n",
    "    print(f\"Total number of parameters: {total_params}\")\n",
    "    print(f\"Model data-type precision: {next(model.parameters()).dtype}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_model_details_and_vram_estimate(model, filename=\"architecture.txt\"):\n",
    "    total_params = 0\n",
    "    total_layers = 0\n",
    "    lines = []\n",
    "    lines.append(\"Model Layers and Parameters:\\n\\n\")\n",
    "    for name, param in model.named_parameters():\n",
    "        total_layers += 1\n",
    "        num_params = param.numel()\n",
    "        total_params += num_params\n",
    "        lines.append(f\"Layer {total_layers}: {name}\\n\")\n",
    "        lines.append(f\" - Shape: {tuple(param.shape)}\\n\")\n",
    "        lines.append(f\" - Data type: {param.dtype}\\n\")\n",
    "        lines.append(f\" - Number of parameters: {num_params}\\n\\n\")\n",
    "    lines.append(f\"Total number of layers: {total_layers}\\n\")\n",
    "    lines.append(f\"Total number of parameters: {total_params}\\n\")\n",
    "    data_type = next(model.parameters()).dtype\n",
    "    lines.append(f\"Model data-type precision: {data_type}\\n\\n\")\n",
    "    \n",
    "    if data_type == torch.float32:\n",
    "        bytes_per_param = 4\n",
    "    elif data_type == torch.float16 or data_type == torch.bfloat16:\n",
    "        bytes_per_param = 2\n",
    "    elif data_type == torch.int8:\n",
    "        bytes_per_param = 1\n",
    "    else:\n",
    "        bytes_per_param = 4  \n",
    "\n",
    "    total_memory_bytes = total_params * bytes_per_param\n",
    "    total_memory_gb = total_memory_bytes / (1024 ** 3)\n",
    "\n",
    "    lines.append(\"Estimated VRAM Required to Hold Model Parameters:\\n\\n\")\n",
    "    lines.append(f\" - Data type: {data_type}\\n\")\n",
    "    lines.append(f\" - Bytes per parameter: {bytes_per_param} bytes\\n\")\n",
    "    lines.append(f\" - Total memory required for parameters: {total_memory_bytes} bytes\\n\")\n",
    "    lines.append(f\" - Total memory required for parameters: {total_memory_gb:.2f} GB\\n\\n\")\n",
    "\n",
    "    # Include considerations for training\n",
    "    lines.append(\"Additional VRAM Considerations for Training:\\n\\n\")\n",
    "    lines.append(\"During training, additional memory is required for gradients, optimizer states, and activations.\\n\")\n",
    "    lines.append(f\" - Memory for gradients: similar to parameters (~{total_memory_gb:.2f} GB)\\n\")\n",
    "    lines.append(\" - Memory for optimizer states: depends on optimizer (e.g., Adam may require up to twice the parameter memory)\\n\")\n",
    "    lines.append(\" - Memory for activations: depends on batch size, sequence length, and model architecture\\n\")\n",
    "    lines.append(\" - Other overheads: CUDA context, framework buffers, etc.\\n\\n\")\n",
    "    lines.append(\"Approximate VRAM Required for Training (excluding activations and overheads):\\n\")\n",
    "    total_training_memory_gb = total_memory_gb * 3  # Parameters + Gradients + Optimizer States\n",
    "    lines.append(f\" - Estimated total memory for parameters, gradients, and optimizer states: {total_training_memory_gb:.2f} GB\\n\")\n",
    "    lines.append(\"Note: Activation memory and other overheads can significantly increase total VRAM usage.\\n\")\n",
    "\n",
    "    with open(filename, 'w') as f:\n",
    "        f.writelines(lines)\n",
    "    print(f\"Model details and VRAM estimation have been written to {filename}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model details and VRAM estimation have been written to architectureBeforeQLoRA.txt\n"
     ]
    }
   ],
   "source": [
    "write_model_details_and_vram_estimate(model,\"architectureBeforeQLoRA.txt\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
