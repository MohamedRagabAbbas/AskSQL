{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForSeq2SeqLM, AutoTokenizer, AutoModelForCausalLM, AutoConfig, Trainer, TrainingArguments, PreTrainedTokenizer, BitsAndBytesConfig\n",
    "from nltk.translate.bleu_score import sentence_bleu\n",
    "import torch\n",
    "import torch.distributed\n",
    "from peft import LoraConfig, PeftModel, get_peft_model, prepare_model_for_kbit_training\n",
    "\n",
    "import copy\n",
    "from dataclasses import dataclass, field\n",
    "from typing import Optional, Dict, Sequence\n",
    "from datasets import load_dataset\n",
    "\n",
    "from typing import List, Dict\n",
    "import random\n",
    "from nltk.translate.bleu_score import sentence_bleu\n",
    "\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LlamaForCausalLM(\n",
       "  (model): LlamaModel(\n",
       "    (embed_tokens): Embedding(32256, 2048)\n",
       "    (layers): ModuleList(\n",
       "      (0-23): 24 x LlamaDecoderLayer(\n",
       "        (self_attn): LlamaSdpaAttention(\n",
       "          (q_proj): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "          (k_proj): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "          (v_proj): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "          (o_proj): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "          (rotary_emb): LlamaRotaryEmbedding()\n",
       "        )\n",
       "        (mlp): LlamaMLP(\n",
       "          (gate_proj): Linear(in_features=2048, out_features=5504, bias=False)\n",
       "          (up_proj): Linear(in_features=2048, out_features=5504, bias=False)\n",
       "          (down_proj): Linear(in_features=5504, out_features=2048, bias=False)\n",
       "          (act_fn): SiLU()\n",
       "        )\n",
       "        (input_layernorm): LlamaRMSNorm((2048,), eps=1e-06)\n",
       "        (post_attention_layernorm): LlamaRMSNorm((2048,), eps=1e-06)\n",
       "      )\n",
       "    )\n",
       "    (norm): LlamaRMSNorm((2048,), eps=1e-06)\n",
       "    (rotary_emb): LlamaRotaryEmbedding()\n",
       "  )\n",
       "  (lm_head): Linear(in_features=2048, out_features=32256, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "MODEL_NAME = \"deepseek-ai/deepseek-coder-1.3b-instruct\"\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    MODEL_NAME,\n",
    "    trust_remote_code=True,\n",
    "    device_map=\"auto\",\n",
    "    torch_dtype=torch.bfloat16)\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "model.to(device)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "ds = load_dataset(\"NumbersStation/NSText2SQL\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_dataset(dataset, eval_ratio=0.1, train_ratio=0.8, test_ratio=0.1):\n",
    "    \"\"\"\n",
    "    Split the Hugging Face dataset into evaluation, training, and test sets.\n",
    "\n",
    "    Parameters:\n",
    "    dataset (Dataset): The dataset loaded using `load_dataset`.\n",
    "    eval_ratio (float): Proportion of the dataset to be used for evaluation.\n",
    "    train_ratio (float): Proportion of the dataset to be used for training.\n",
    "    test_ratio (float): Proportion of the dataset to be used for testing.\n",
    "\n",
    "    Returns:\n",
    "    tuple: A tuple containing (train_set, eval_set, test_set).\n",
    "    \"\"\"\n",
    "    assert eval_ratio + train_ratio + test_ratio == 1, \"Ratios must sum to 1\"\n",
    "    train_test_split = dataset.train_test_split(test_size=(eval_ratio + test_ratio))\n",
    "    eval_test_split = train_test_split['test'].train_test_split(test_size=(test_ratio / (eval_ratio + test_ratio)))\n",
    "    return train_test_split['train'], eval_test_split['train'], eval_test_split['test']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Set Size: 231430\n",
      "Evaluation Set Size: 28929\n",
      "Test Set Size: 28929\n"
     ]
    }
   ],
   "source": [
    "train_split = ds['train']\n",
    "\n",
    "train_set, eval_set, test_set = split_dataset(train_split)\n",
    "\n",
    "print(\"Training Set Size:\", len(train_set))\n",
    "print(\"Evaluation Set Size:\", len(eval_set))\n",
    "print(\"Test Set Size:\", len(test_set))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_datasets_by_source(train_set, eval_set, test_set, source_value='spider'):\n",
    "    \"\"\"\n",
    "    Filters the input datasets based on the 'source' column matching the given source_value.\n",
    "\n",
    "    Args:\n",
    "        train_set: The training dataset.\n",
    "        eval_set: The evaluation dataset.\n",
    "        test_set: The test dataset.\n",
    "        source_value (str): The value in the 'source' column to filter by (default is 'spider').\n",
    "\n",
    "    Returns:\n",
    "        tuple: A tuple containing the filtered train_set, eval_set, and test_set.\n",
    "    \"\"\"\n",
    "    filtered_train_set = train_set.filter(lambda example: example['source'] == source_value)\n",
    "    filtered_eval_set = eval_set.filter(lambda example: example['source'] == source_value)\n",
    "    filtered_test_set = test_set.filter(lambda example: example['source'] == source_value)\n",
    "    \n",
    "    return filtered_train_set, filtered_eval_set, filtered_test_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0bf4fd73ced641c0b6f5985601bb5019",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Filter:   0%|          | 0/231430 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cfcc4e2ec2534ca68067f3e39fde2e89",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Filter:   0%|          | 0/28929 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "52555e5cd12c432b8d9d2178e467a99a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Filter:   0%|          | 0/28929 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "spider_train, spider_eval,spider_test = filter_datasets_by_source(train_set, eval_set, test_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Set Size: 5564\n",
      "Evaluation Set Size: 721\n",
      "Test Set Size: 709\n"
     ]
    }
   ],
   "source": [
    "print(\"Training Set Size:\", len(spider_train))\n",
    "print(\"Evaluation Set Size:\", len(spider_eval))\n",
    "print(\"Test Set Size:\", len(spider_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['instruction', 'output', 'source'],\n",
       "    num_rows: 709\n",
       "})"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spider_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "import torch\n",
    "from nltk.translate.bleu_score import sentence_bleu\n",
    "import random\n",
    "from transformers import AutoModelForSeq2SeqLM, AutoTokenizer\n",
    "\n",
    "def evaluate_bleu(\n",
    "    model: AutoModelForSeq2SeqLM,\n",
    "    tokenizer: AutoTokenizer,\n",
    "    test_data,  \n",
    "    batch_size: int = 1,\n",
    "    eval_percentage: float = 1.0,\n",
    "    max_new_tokens: int = 50,\n",
    ") -> float:\n",
    "    \"\"\"\n",
    "    Evaluate the model using BLEU score on a subset of the test dataset.\n",
    "\n",
    "    Parameters:\n",
    "        model (AutoModelForSeq2SeqLM): The pre-trained model for evaluation.\n",
    "        tokenizer (AutoTokenizer): The tokenizer for the model.\n",
    "        test_data: The test dataset containing 'instruction' and 'output' keys.\n",
    "        batch_size (int): The number of samples to process at once.\n",
    "        eval_percentage (float): The percentage of the test data to use for evaluation (0 to 1).\n",
    "        max_new_tokens (int): The maximum number of tokens to generate for each prediction.\n",
    "        device (str): The device to run the model on ('cuda' or 'cpu').\n",
    "\n",
    "    Returns:\n",
    "        float: The average BLEU score over the evaluated subset.\n",
    "    \"\"\"\n",
    "    if tokenizer.pad_token_id is None:\n",
    "        tokenizer.pad_token_id = tokenizer.eos_token_id\n",
    "\n",
    "    test_data = list(test_data)\n",
    "\n",
    "    model = model.to(device)\n",
    "\n",
    "    eval_data = random.sample(test_data, int(len(test_data) * eval_percentage))\n",
    "\n",
    "    bleu_scores = []\n",
    "    with torch.no_grad():\n",
    "        for i in tqdm(range(0, len(eval_data), batch_size), desc=\"Evaluating BLEU score\"):\n",
    "            batch = eval_data[i:i + batch_size]\n",
    "            batch_instructions = [sample[\"instruction\"] for sample in batch]\n",
    "            batch_outputs = [sample[\"output\"] for sample in batch]\n",
    "            inputs = tokenizer(\n",
    "                batch_instructions,\n",
    "                return_tensors=\"pt\",\n",
    "                padding=True,\n",
    "                truncation=True\n",
    "            ).to(device)\n",
    "            generated_outputs = model.generate(\n",
    "                inputs[\"input_ids\"], \n",
    "                attention_mask=inputs[\"attention_mask\"], \n",
    "                max_new_tokens=max_new_tokens\n",
    "            )\n",
    "            for j, output in enumerate(generated_outputs):\n",
    "                predicted_text = tokenizer.decode(output, skip_special_tokens=True)\n",
    "                reference = [batch_outputs[j].split()]  \n",
    "                hypothesis = predicted_text.split()     \n",
    "                score = sentence_bleu(reference, hypothesis)\n",
    "                \n",
    "                bleu_scores.append(score)\n",
    "    average_bleu = sum(bleu_scores) / len(bleu_scores) if bleu_scores else 0.0\n",
    "    print(f\"Average BLEU score: {average_bleu}\")\n",
    "    return average_bleu\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating BLEU score:   0%|          | 0/9 [00:00<?, ?it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "/home/youssefm/.local/lib/python3.8/site-packages/nltk/translate/bleu_score.py:577: UserWarning: \n",
      "The hypothesis contains 0 counts of 3-gram overlaps.\n",
      "Therefore the BLEU score evaluates to 0, independently of\n",
      "how many N-gram overlaps of lower order it contains.\n",
      "Consider using lower n-gram order or use SmoothingFunction()\n",
      "  warnings.warn(_msg)\n",
      "/home/youssefm/.local/lib/python3.8/site-packages/nltk/translate/bleu_score.py:577: UserWarning: \n",
      "The hypothesis contains 0 counts of 4-gram overlaps.\n",
      "Therefore the BLEU score evaluates to 0, independently of\n",
      "how many N-gram overlaps of lower order it contains.\n",
      "Consider using lower n-gram order or use SmoothingFunction()\n",
      "  warnings.warn(_msg)\n",
      "/home/youssefm/.local/lib/python3.8/site-packages/nltk/translate/bleu_score.py:577: UserWarning: \n",
      "The hypothesis contains 0 counts of 2-gram overlaps.\n",
      "Therefore the BLEU score evaluates to 0, independently of\n",
      "how many N-gram overlaps of lower order it contains.\n",
      "Consider using lower n-gram order or use SmoothingFunction()\n",
      "  warnings.warn(_msg)\n",
      "Evaluating BLEU score:  11%|█         | 1/9 [01:00<08:00, 60.04s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Evaluating BLEU score:  22%|██▏       | 2/9 [01:58<06:54, 59.18s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Evaluating BLEU score:  33%|███▎      | 3/9 [03:05<06:16, 62.82s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Evaluating BLEU score:  44%|████▍     | 4/9 [04:05<05:08, 61.78s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Evaluating BLEU score:  56%|█████▌    | 5/9 [05:53<05:13, 78.41s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Evaluating BLEU score:  67%|██████▋   | 6/9 [06:55<03:37, 72.64s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Evaluating BLEU score:  78%|███████▊  | 7/9 [08:02<02:21, 70.95s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Evaluating BLEU score:  89%|████████▉ | 8/9 [09:10<01:09, 69.89s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Evaluating BLEU score: 100%|██████████| 9/9 [10:06<00:00, 67.40s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average BLEU score: 0.006862448861117331\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "average_bleu_score = evaluate_bleu(\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    test_data=spider_test,\n",
    "    batch_size=8,\n",
    "    eval_percentage=0.1,  \n",
    "    max_new_tokens=500\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_response(\n",
    "    model: AutoModelForSeq2SeqLM,\n",
    "    tokenizer: AutoTokenizer,\n",
    "    query: str,\n",
    "    max_new_tokens: int = 50\n",
    ") -> str:\n",
    "    \"\"\"\n",
    "    Generates a response from the model given a query.\n",
    "\n",
    "    Parameters:\n",
    "        model (AutoModelForSeq2SeqLM): The pre-trained model.\n",
    "        tokenizer (AutoTokenizer): The tokenizer for the model.\n",
    "        query (str): The input query string.\n",
    "        max_new_tokens (int): The maximum number of tokens to generate for the response.\n",
    "        device (str): The device to run the model on ('cuda' if available, otherwise 'cpu').\n",
    "\n",
    "    Returns:\n",
    "        str: The generated response text.\n",
    "    \"\"\"\n",
    "    model = model.to(device)\n",
    "    inputs = tokenizer(query, return_tensors=\"pt\").to(device)\n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(\n",
    "            inputs[\"input_ids\"],\n",
    "            attention_mask=inputs[\"attention_mask\"],\n",
    "            max_new_tokens=max_new_tokens,\n",
    "            pad_token_id=tokenizer.pad_token_id\n",
    "        )\n",
    "    response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    \n",
    "    return response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated Response: CREATE TABLE people (\n",
      "    people_id number,\n",
      "    name text,\n",
      "    age number,\n",
      "    height number,\n",
      "    hometown text\n",
      ")\n",
      "\n",
      "CREATE TABLE gymnast (\n",
      "    gymnast_id number,\n",
      "    floor_exercise_points number,\n",
      "    pommel_horse_points number,\n",
      "    rings_points number,\n",
      "    vault_points number,\n",
      "    parallel_bars_points number,\n",
      "    horizontal_bar_points number,\n",
      "    total_points number\n",
      ")\n",
      "\n",
      "\n",
      "-- Using valid SQLite, answer the following questions for the tables provided above.\n",
      "\n",
      "-- What are the hometowns that are shared by at least two gymnasts?\n",
      "SELECT hometown\n",
      "FROM gymnast\n",
      "GROUP BY hometown\n",
      "HAVING COUNT(*) >= 2\n",
      "\n",
      "-- What is the average age of a gymnast?\n",
      "SELECT AVG(age)\n",
      "FROM gymnast\n"
     ]
    }
   ],
   "source": [
    "query = spider_test['instruction'][0]\n",
    "response = generate_response(model, tokenizer, query)\n",
    "print(\"Generated Response:\", response)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cuda-gpt",
   "language": "python",
   "name": "cuda"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
